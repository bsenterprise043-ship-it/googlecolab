{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bsenterprise043-ship-it/googlecolab/blob/main/Qwen3_TTS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ðŸ·ï¸ **Credits & License**\n",
        "\n",
        "* ðŸ”— [Qwen3-TTS GitHub Repository](https://github.com/QwenLM/Qwen3-TTS)\n",
        "* ðŸ¤— [Qwen3-TTS on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-tts)\n",
        "* ðŸ“„ **License**: Provided under the [Apache License 2.0](https://github.com/QwenLM/Qwen3-TTS?tab=Apache-2.0-1-ov-file)\n",
        "* ðŸ¤— [Try Qwen3-TTS on HuggingFace Space](https://huggingface.co/spaces/Qwen/Qwen3-TTS)\n",
        "\n",
        "\n",
        "\n",
        "### âš ï¸ **Usage Disclaimer**\n",
        "\n",
        "Use of this voice cloning model is subject to strict ethical and legal standards. By using this tool, you agree **not to** engage in any of the following prohibited activities:\n",
        "\n",
        "* **Fraud or Deception**: Using cloned voices to create misleading or fraudulent content.\n",
        "* **Impersonation**: Replicating someoneâ€™s voice without their explicit permission, especially for malicious, harmful, or deceptive purposes.\n",
        "* **Illegal Activities**: Employing the model in any manner that violates local, national, or international laws and regulations.\n",
        "* **Harmful Content Generation**: Creating offensive, defamatory, or unethical material, including content that spreads misinformation or causes harm.\n",
        "\n",
        "> âš–ï¸ **Legal Responsibility**\n",
        "> The developers of this tool disclaim all liability for misuse. **Users bear full responsibility** for ensuring that their usage complies with all applicable laws, regulations, and ethical guidelines.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O5hhJS2moOhU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "57sW-0cHjthT"
      },
      "outputs": [],
      "source": [
        "#@title Install Qwen3-TTS\n",
        "%cd /content/\n",
        "# !rm -rf /content/Qwen3-TTS-Colab\n",
        "!git clone https://github.com/NeuralFalconYT/Qwen3-TTS-Colab.git\n",
        "!git clone https://github.com/QwenLM/Qwen3-TTS.git\n",
        "%cd Qwen3-TTS\n",
        "!pip install -e .\n",
        "!pip install faster-whisper==1.1.1\n",
        "!pip install ctranslate2==4.5.0\n",
        "!pip install pysrt\n",
        "!pip install sentencex\n",
        "from IPython.display import Audio,display\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "clear_output()\n",
        "\n",
        "display(Audio(\"https://raw.githubusercontent.com/NeuralFalconYT/Useful-Function/refs/heads/main/audio/warning.mp3\", autoplay=True))\n",
        "time.sleep(6)\n",
        "clear_output()\n",
        "# time.sleep(5)\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Qwen3-TTS-Colab\n",
        "!python app.py --share --debug"
      ],
      "metadata": {
        "id": "v7Y8L5EDpYNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bb22c1b-6690-49c9-9cfc-1a1a32b3270f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Qwen3-TTS-Colab\n",
            "2026-02-10 13:53:35.354318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1770731615.373806     652 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1770731615.379995     652 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1770731615.395423     652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770731615.395449     652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770731615.395453     652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1770731615.395456     652 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-02-10 13:53:35.400005: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "********\n",
            "Warning: flash-attn is not installed. Will only run the manual PyTorch version. Please install flash-attn for faster inference.\n",
            "********\n",
            " \n",
            "/bin/sh: 1: sox: not found\n",
            "WARNING:sox:SoX could not be found!\n",
            "\n",
            "    If you do not have SoX, proceed here:\n",
            "     - - - http://sox.sourceforge.net/ - - -\n",
            "\n",
            "    If you do (or think that you should) have SoX, double-check your\n",
            "    path variables.\n",
            "    \n",
            "/content/Qwen3-TTS-Colab/app.py:354: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=theme, css=css, title=\"Qwen3-TTS Demo\") as demo:\n",
            "/content/Qwen3-TTS-Colab/app.py:354: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=theme, css=css, title=\"Qwen3-TTS Demo\") as demo:\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://81250a01f0adc849ab.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "\u001b[33mWARNING\u001b[0m:  Invalid HTTP request received.\n",
            "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 416, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1139, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 107, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 888, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 904, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 119, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 105, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 385, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 284, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1708, in upload_file\n",
            "    form = await multipart_parser.parse()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 742, in parse\n",
            "    async for chunk in self.stream:\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/requests.py\", line 237, in stream\n",
            "    raise ClientDisconnect()\n",
            "starlette.requests.ClientDisconnect\n",
            "\u001b[31mERROR\u001b[0m:    Exception in ASGI application\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 416, in run_asgi\n",
            "    result = await app(  # type: ignore[func-returns-value]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/applications.py\", line 1139, in __call__\n",
            "    await super().__call__(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/applications.py\", line 107, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 186, in __call__\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/errors.py\", line 164, in __call__\n",
            "    await self.app(scope, receive, _send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/brotli_middleware.py\", line 74, in __call__\n",
            "    return await self.app(scope, receive, send)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 888, in __call__\n",
            "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 904, in simple_response\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/middleware/exceptions.py\", line 63, in __call__\n",
            "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/middleware/asyncexitstack.py\", line 18, in __call__\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 716, in __call__\n",
            "    await self.middleware_stack(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 736, in app\n",
            "    await route.handle(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/routing.py\", line 290, in handle\n",
            "    await self.app(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 119, in app\n",
            "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n",
            "    raise exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n",
            "    await app(scope, receive, sender)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 105, in app\n",
            "    response = await f(request)\n",
            "               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 385, in app\n",
            "    raw_response = await run_endpoint_function(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/fastapi/routing.py\", line 284, in run_endpoint_function\n",
            "    return await dependant.call(**values)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/routes.py\", line 1708, in upload_file\n",
            "    form = await multipart_parser.parse()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 742, in parse\n",
            "    async for chunk in self.stream:\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/starlette/requests.py\", line 237, in stream\n",
            "    raise ClientDisconnect()\n",
            "starlette.requests.ClientDisconnect\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "README.md: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "config.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "config.json: 4.49kB [00:00, 3.00MB/s]\n",
            "generation_config.json: 100% 245/245 [00:00<00:00, 468kB/s]\n",
            "README.md: 3.64kB [00:00, 989kB/s]\n",
            "\n",
            "\n",
            "config.json: 0.00B [00:00, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "preprocessor_config.json:   0% 0.00/127 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "preprocessor_config.json: 100% 127/127 [00:00<00:00, 59.1kB/s]\n",
            "config.json: 2.34kB [00:00, 391kB/s]\n",
            ".gitattributes: 1.52kB [00:00, 544kB/s]\n",
            "merges.txt: 1.67MB [00:00, 28.9MB/s]\n",
            "\n",
            "model.safetensors:   0% 0.00/1.83G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "configuration.json: 100% 76.0/76.0 [00:00<00:00, 438kB/s]\n",
            "\n",
            "\n",
            "tokenizer_config.json: 7.34kB [00:00, 5.17MB/s]\n",
            "\n",
            "\n",
            "preprocessor_config.json: 100% 234/234 [00:00<00:00, 1.92MB/s]\n",
            "\n",
            "\n",
            "speech_tokenizer/model.safetensors:   0% 0.00/682M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "vocab.json: 2.78MB [00:00, 59.5MB/s]\n",
            "\n",
            "\n",
            "speech_tokenizer/model.safetensors:   1% 7.34M/682M [00:00<00:44, 15.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:   7% 45.3M/682M [00:00<00:10, 62.3MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   0% 807k/1.83G [00:01<38:40, 788kB/s]\u001b[A\n",
            "model.safetensors:   0% 1.73M/1.83G [00:01<20:55, 1.46MB/s]\u001b[A\n",
            "model.safetensors:   0% 2.80M/1.83G [00:01<12:32, 2.43MB/s]\u001b[A\n",
            "model.safetensors:   1% 19.4M/1.83G [00:03<04:27, 6.77MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  16% 112M/682M [00:04<00:23, 24.3MB/s] \u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  26% 179M/682M [00:04<00:11, 43.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  36% 247M/682M [00:04<00:06, 67.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  46% 314M/682M [00:05<00:05, 66.9MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   5% 86.4M/1.83G [00:06<01:43, 16.8MB/s]\u001b[A\n",
            "model.safetensors:   8% 153M/1.83G [00:06<00:51, 32.7MB/s] \u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  56% 381M/682M [00:07<00:05, 52.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  66% 448M/682M [00:09<00:05, 42.9MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  12% 221M/1.83G [00:10<01:00, 26.5MB/s]\u001b[A\n",
            "model.safetensors:  16% 288M/1.83G [00:10<00:37, 41.3MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  75% 515M/682M [00:10<00:03, 54.9MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  19% 355M/1.83G [00:10<00:27, 54.3MB/s]\u001b[A\n",
            "model.safetensors:  23% 422M/1.83G [00:10<00:17, 78.2MB/s]\u001b[A\n",
            "model.safetensors:  27% 489M/1.83G [00:10<00:12, 109MB/s] \u001b[A\n",
            "model.safetensors:  30% 556M/1.83G [00:11<00:09, 130MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  85% 582M/682M [00:11<00:01, 56.7MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  34% 623M/1.83G [00:11<00:08, 135MB/s]\u001b[A\n",
            "model.safetensors:  38% 690M/1.83G [00:11<00:07, 157MB/s]\u001b[A\n",
            "model.safetensors:  41% 757M/1.83G [00:14<00:14, 71.8MB/s]\u001b[A\n",
            "model.safetensors:  49% 891M/1.83G [00:14<00:08, 116MB/s] \u001b[A\n",
            "model.safetensors:  52% 958M/1.83G [00:14<00:06, 138MB/s]\u001b[A\n",
            "model.safetensors:  56% 1.02G/1.83G [00:14<00:05, 159MB/s]\u001b[A\n",
            "model.safetensors:  60% 1.09G/1.83G [00:15<00:04, 175MB/s]\u001b[A\n",
            "model.safetensors:  63% 1.16G/1.83G [00:15<00:03, 189MB/s]\u001b[A\n",
            "model.safetensors:  67% 1.23G/1.83G [00:15<00:03, 175MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors: 100% 682M/682M [00:16<00:00, 40.6MB/s]\n",
            "\n",
            "model.safetensors:  71% 1.29G/1.83G [00:18<00:07, 71.3MB/s]\u001b[A\n",
            "model.safetensors:  78% 1.43G/1.83G [00:18<00:03, 112MB/s] \u001b[A\n",
            "model.safetensors:  82% 1.49G/1.83G [00:18<00:02, 135MB/s]\u001b[A\n",
            "model.safetensors:  85% 1.56G/1.83G [00:19<00:01, 149MB/s]\u001b[A\n",
            "model.safetensors:  89% 1.63G/1.83G [00:19<00:01, 182MB/s]\u001b[A\n",
            "model.safetensors:  93% 1.70G/1.83G [00:19<00:00, 213MB/s]\u001b[A\n",
            "model.safetensors:  96% 1.76G/1.83G [00:19<00:00, 229MB/s]\u001b[A\n",
            "model.safetensors: 100% 1.83G/1.83G [00:19<00:00, 92.2MB/s]\n",
            "Fetching 13 files: 100% 13/13 [00:20<00:00,  1.55s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
            "Stitching 1 audio files...\n",
            "Starting transcription for: /tmp/gradio/d103561ceae618e86cb803baf84edb5946f4964e62e98de4f883083507a9142f/Morgan Freemans Karen.wav\n",
            "config.json: 2.26kB [00:00, 14.3MB/s]\n",
            "tokenizer.json: 0.00B [00:00, ?B/s]\n",
            "preprocessor_config.json: 100% 340/340 [00:00<00:00, 3.18MB/s]\n",
            "\n",
            "tokenizer.json: 2.71MB [00:00, 75.9MB/s]\n",
            "vocabulary.json: 1.07MB [00:00, 42.6MB/s]\n",
            "model.bin: 100% 1.62G/1.62G [00:29<00:00, 55.0MB/s]\n",
            "Starting transcription for: /tmp/gradio/d103561ceae618e86cb803baf84edb5946f4964e62e98de4f883083507a9142f/Morgan Freemans Karen.wav\n",
            "Fetching 13 files:   0% 0/13 [00:00<?, ?it/s]\n",
            "config.json: 4.49kB [00:00, 12.5MB/s]\n",
            "\n",
            "config.json: 2.34kB [00:00, 3.03MB/s]\n",
            "\n",
            "merges.txt: 0.00B [00:00, ?B/s]\u001b[A\n",
            "\n",
            "generation_config.json: 100% 245/245 [00:00<00:00, 2.21MB/s]\n",
            "\n",
            "\n",
            ".gitattributes: 1.52kB [00:00, 12.3MB/s]\n",
            "Fetching 13 files:   8% 1/13 [00:00<00:01,  7.41it/s]\n",
            "\n",
            "README.md: 57.8kB [00:00, 108MB/s]\n",
            "\n",
            "\n",
            "preprocessor_config.json: 100% 127/127 [00:00<00:00, 938kB/s]\n",
            "merges.txt: 1.67MB [00:00, 47.7MB/s]\n",
            "\n",
            "model.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "preprocessor_config.json: 100% 234/234 [00:00<00:00, 1.60MB/s]\n",
            "\n",
            "\n",
            "configuration.json: 100% 76.0/76.0 [00:00<00:00, 73.8kB/s]\n",
            "\n",
            "\n",
            "tokenizer_config.json: 7.34kB [00:00, 46.6MB/s]\n",
            "\n",
            "\n",
            "vocab.json: 2.78MB [00:00, 63.4MB/s]\n",
            "\n",
            "\n",
            "speech_tokenizer/model.safetensors:   0% 0.00/682M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:   1% 7.34M/682M [00:00<00:47, 14.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:   7% 45.3M/682M [00:00<00:08, 79.6MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   0% 808k/3.86G [00:00<1:14:05, 868kB/s]\u001b[A\n",
            "model.safetensors:   0% 2.24M/3.86G [00:02<1:05:29, 981kB/s]\u001b[A\n",
            "model.safetensors:   2% 69.2M/3.86G [00:05<04:12, 15.0MB/s] \u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  16% 112M/682M [00:05<00:31, 18.0MB/s] \u001b[A\u001b[A\n",
            "model.safetensors:   4% 136M/3.86G [00:06<02:24, 25.8MB/s] \u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  16% 112M/682M [00:20<00:31, 18.0MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   4% 136M/3.86G [00:20<02:24, 25.8MB/s]\u001b[A\n",
            "model.safetensors:   5% 203M/3.86G [00:51<19:55, 3.06MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  26% 179M/682M [00:52<03:06, 2.70MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:   7% 270M/3.86G [00:54<12:53, 4.64MB/s]\u001b[A\n",
            "model.safetensors:   9% 337M/3.86G [00:58<09:11, 6.39MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  26% 179M/682M [01:02<03:06, 2.70MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  10% 405M/3.86G [01:04<07:43, 7.45MB/s]\u001b[A\n",
            "model.safetensors:  12% 474M/3.86G [01:04<05:06, 11.0MB/s]\u001b[A\n",
            "model.safetensors:  14% 541M/3.86G [01:05<03:32, 15.6MB/s]\u001b[A\n",
            "model.safetensors:  16% 608M/3.86G [01:06<02:40, 20.2MB/s]\u001b[A\n",
            "model.safetensors:  17% 675M/3.86G [01:10<02:52, 18.4MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  36% 246M/682M [01:10<02:24, 3.02MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  19% 742M/3.86G [01:11<02:05, 24.7MB/s]\u001b[A\n",
            "model.safetensors:  21% 809M/3.86G [01:16<02:42, 18.8MB/s]\u001b[A\n",
            "model.safetensors:  23% 876M/3.86G [01:16<01:52, 26.4MB/s]\u001b[A\n",
            "model.safetensors:  24% 943M/3.86G [01:22<02:35, 18.7MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  36% 246M/682M [01:22<02:24, 3.02MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  26% 1.01G/3.86G [01:22<01:47, 26.4MB/s]\u001b[A\n",
            "model.safetensors:  28% 1.08G/3.86G [01:28<02:23, 19.3MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  46% 314M/682M [01:28<01:53, 3.25MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  30% 1.14G/3.86G [01:29<01:45, 25.8MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  56% 381M/682M [01:29<01:01, 4.90MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  31% 1.21G/3.86G [01:35<02:22, 18.6MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  71% 481M/682M [01:36<00:29, 6.82MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  33% 1.28G/3.86G [01:46<03:45, 11.4MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  71% 481M/682M [01:50<00:29, 6.82MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  35% 1.35G/3.86G [01:57<04:41, 8.91MB/s]\u001b[A\n",
            "model.safetensors:  37% 1.41G/3.86G [02:07<05:04, 8.04MB/s]\u001b[A\n",
            "model.safetensors:  38% 1.48G/3.86G [02:14<04:32, 8.73MB/s]\u001b[A\n",
            "model.safetensors:  40% 1.55G/3.86G [02:18<03:56, 9.77MB/s]\u001b[A\n",
            "model.safetensors:  41% 1.58G/3.86G [02:20<03:24, 11.1MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  80% 548M/682M [02:20<00:39, 3.40MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  43% 1.65G/3.86G [02:20<02:17, 16.1MB/s]\u001b[A\n",
            "model.safetensors:  44% 1.72G/3.86G [02:20<01:32, 23.2MB/s]\u001b[A\n",
            "model.safetensors:  46% 1.78G/3.86G [02:21<01:04, 32.4MB/s]\u001b[A\n",
            "model.safetensors:  48% 1.85G/3.86G [02:26<01:31, 22.1MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors:  90% 615M/682M [02:26<00:15, 4.26MB/s]\u001b[A\u001b[A\n",
            "model.safetensors:  50% 1.92G/3.86G [02:26<01:03, 30.4MB/s]\u001b[A\n",
            "model.safetensors:  51% 1.98G/3.86G [02:26<00:45, 41.0MB/s]\u001b[A\n",
            "model.safetensors:  53% 2.05G/3.86G [02:27<00:32, 55.4MB/s]\u001b[A\n",
            "\n",
            "speech_tokenizer/model.safetensors: 100% 682M/682M [02:27<00:00, 4.63MB/s]\n",
            "\n",
            "model.safetensors:  55% 2.12G/3.86G [02:27<00:26, 64.7MB/s]\u001b[A\n",
            "model.safetensors:  57% 2.18G/3.86G [02:28<00:20, 82.3MB/s]\u001b[A\n",
            "model.safetensors:  58% 2.25G/3.86G [02:29<00:26, 61.1MB/s]\u001b[A\n",
            "model.safetensors:  60% 2.32G/3.86G [02:30<00:19, 80.4MB/s]\u001b[A\n",
            "model.safetensors:  62% 2.39G/3.86G [02:30<00:16, 91.0MB/s]\u001b[A\n",
            "model.safetensors:  64% 2.45G/3.86G [02:30<00:11, 120MB/s] \u001b[A\n",
            "model.safetensors:  65% 2.52G/3.86G [02:31<00:09, 142MB/s]\u001b[A\n",
            "model.safetensors:  67% 2.59G/3.86G [02:31<00:07, 173MB/s]\u001b[A\n",
            "model.safetensors:  69% 2.65G/3.86G [02:36<00:33, 35.9MB/s]\u001b[A\n",
            "model.safetensors:  71% 2.72G/3.86G [02:36<00:23, 49.3MB/s]\u001b[A\n",
            "model.safetensors:  72% 2.79G/3.86G [02:36<00:15, 67.0MB/s]\u001b[A\n",
            "model.safetensors:  74% 2.85G/3.86G [02:37<00:11, 86.0MB/s]\u001b[A\n",
            "model.safetensors:  76% 2.92G/3.86G [02:37<00:08, 105MB/s] \u001b[A\n",
            "model.safetensors:  77% 2.99G/3.86G [02:37<00:06, 131MB/s]\u001b[A\n",
            "model.safetensors:  79% 3.05G/3.86G [02:38<00:07, 113MB/s]\u001b[A\n",
            "model.safetensors:  81% 3.12G/3.86G [02:40<00:11, 61.6MB/s]\u001b[A\n",
            "model.safetensors:  83% 3.19G/3.86G [02:41<00:08, 75.0MB/s]\u001b[A\n",
            "model.safetensors:  84% 3.25G/3.86G [02:46<00:21, 28.7MB/s]\u001b[A\n",
            "model.safetensors:  86% 3.32G/3.86G [02:47<00:13, 38.7MB/s]\u001b[A\n",
            "model.safetensors:  88% 3.39G/3.86G [02:50<00:16, 28.7MB/s]\u001b[A\n",
            "model.safetensors:  91% 3.52G/3.86G [02:51<00:06, 50.1MB/s]\u001b[A\n",
            "model.safetensors:  93% 3.59G/3.86G [02:51<00:04, 63.2MB/s]\u001b[A\n",
            "model.safetensors:  95% 3.66G/3.86G [02:51<00:02, 79.0MB/s]\u001b[A\n",
            "model.safetensors:  97% 3.72G/3.86G [02:51<00:01, 97.4MB/s]\u001b[A\n",
            "model.safetensors:  98% 3.79G/3.86G [02:52<00:00, 118MB/s] \u001b[A\n",
            "model.safetensors: 100% 3.86G/3.86G [02:52<00:00, 22.4MB/s]\n",
            "Fetching 13 files: 100% 13/13 [02:52<00:00, 13.28s/it]\n",
            "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
            "Stitching 1 audio files...\n",
            "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
            "Stitching 1 audio files...\n",
            "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
            "Stitching 1 audio files...\n",
            "Setting `pad_token_id` to `eos_token_id`:2150 for open-end generation.\n",
            "Stitching 1 audio files...\n"
          ]
        }
      ]
    }
  ]
}